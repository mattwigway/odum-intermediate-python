{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad677e55-f101-4785-8eab-c63ffb91664a",
   "metadata": {},
   "source": [
    "# Tabular data manipulation\n",
    "\n",
    "In the Introduction to Python class, we spent a good deal of time learning to manipulate tabular data with the `pandas` library and create plots with `matplotlib`. In this exercise, we will recap loading, manipulating, and plotting tabular data.\n",
    "\n",
    "As always, we first need to load the libraries we will be usingâ€”just `pandas` and `matplotlib` in this case, and we're giving them the conventional aliases `pd` and `plt` to save typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd15bc-cee8-42d5-b060-1b57143f1aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1eea3-0ec3-4bbd-9cd6-53bf7e91e83f",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "As before, we'll use `pandas` `read_csv` function to read a CSV file. The data we have here are data on traffic counts in New York City, from the [New York City open data portal](https://data.cityofnewyork.us/Transportation/Traffic-Volume-Counts/btm5-ppia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1902908b-2dac-4096-bc9e-8ab4e3f017c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Traffic_Volume_Counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155c0c9-930d-4c19-aea6-caca72376ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e52e30-45ad-481d-b33c-94365149c5c4",
   "metadata": {},
   "source": [
    "In general, I'm not a big fan of column names with special characters in them. We can leave the time columns as they are for now, but let's rename the roadway name column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229b1ca-6e5d-4466-a02b-36421d6a5dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Roadway Name\": \"roadway_name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0873f5d-0505-4423-a266-a5a28b06411e",
   "metadata": {},
   "source": [
    "## Joins\n",
    "\n",
    "Oftentimes, the data you want will be in multiple files that you have to join together based on some common identifier. In this case, we'd like to know what borough of New York City each of these segments are in. That is in the `../data/Traffic_Borough.csv` file, extracted from the [New York City LION geospatial database](https://www.nyc.gov/site/planning/data-maps/open-data/dwn-lion.page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bf91da-9feb-4e38-a1ec-d18e99ba7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = pd.read_csv(\"../data/Traffic_Borough.csv\")\n",
    "borough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083fc5d6-4d27-4c72-b5af-3da2b5349cd3",
   "metadata": {},
   "source": [
    "What column would we use to link this data to the original dataset?\n",
    "\n",
    "The `RBoro` column contains numeric codes for the five boroughs, but for our analysis, we want strings.\n",
    "\n",
    "The codes are 1 for Manhattan, 2 for the Bronx, 3 for Brooklyn, 4 for Queens, and 5 for Staten Island. We can use the `.recode` function to create a new `borough` field in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55082818-83dd-4982-b8c1-ed553bc1a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough[\"borough\"] = borough.RBoro.replace({\n",
    "    1: \"Manhattan\",\n",
    "    2: \"Bronx\",\n",
    "    3: \"Brooklyn\",\n",
    "    4: \"Queens\",\n",
    "    5: \"Staten Island\"\n",
    "}).astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064bc55a-8e6a-4059-adf6-41044b3bc759",
   "metadata": {},
   "source": [
    "Next, we can join the data with the borough file. We use the `.merge` function to do this. Recall that there are left, right, inner, and outer joins; in this case we'll specify we want a left join. I always recommend validating your joins as well, to make sure the data match the way you expect them to. In this case, this should be a many-to-one join: there should be multiple records per segment in the traffic data, and only a single record per segment in the boroughs file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905d52b2-8929-4efc-821c-56c6d5f9c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(borough, on=\"SegmentID\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b5f12-10c2-4eac-a4d7-566987745149",
   "metadata": {},
   "source": [
    "Well, we got an error - this is why I like to validate my merges. The error message notes that the merge keys are not unique in the right dataset, and therefore this is not a many-to-one join. Let's investigate which ones are duplicated. keep=False tells pandas to show us all duplicated records - usually it would only show us the ones after the first duplicated record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9954ea-f882-4210-b4ff-4d0483002649",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough[borough.SegmentID.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a44c20-ca1f-4770-b281-f3351803e705",
   "metadata": {},
   "source": [
    "Many of these look like they are true duplicates in the data - for instance, segment 77356 is recorded four times, but they all consistently say this segment is in the Bronx. We can use the `.drop_duplicates` function to drop the true duplicate rows in the borough dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476fba84-1a5b-461b-8cbf-dc570dd0cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = borough.drop_duplicates()\n",
    "data = data.merge(borough, on=\"SegmentID\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b946246-3d53-4bb8-9018-5bece2207f2c",
   "metadata": {},
   "source": [
    "We still get the same error. What does that mean?\n",
    "\n",
    "That must mean there were some duplicated segment IDs that _didn't_ have the same borough. Let's take a look at the duplicates again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcd95e8-edd3-4b0a-9a54-5170cd295a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough[borough.SegmentID.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3432513-94bd-445f-8547-971e83f43439",
   "metadata": {},
   "source": [
    "I don't see any duplicated segment IDs in the preview, which must mean the duplicates are not in order. Let's look at a specific case, segment 69424."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba6f76-bfe5-42e7-a22b-53a8b41ae474",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough.loc[borough.SegmentID == 69424]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0195b2-8ec2-4037-b785-8142ebbdff17",
   "metadata": {},
   "source": [
    "Ah, one record has NaN values. We don't have any use for NaN borough names, so we can drop those. Hopefully this will solve our duplicates problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cd08f3-8c87-4e58-9d3c-b76a2bca96e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = borough.dropna(subset=\"borough\")\n",
    "borough.loc[borough.SegmentID.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75019c-c6b3-42ac-a130-6572dfd46fbe",
   "metadata": {},
   "source": [
    "Good, there are no more duplicate segment IDs. Now, let's just add an assertion that that is the case - this is just an automated check that some assumption we made is true. The check will be run every time we run the notebook, and will cause an error if it ever stops being true (for example, due to a code change above).\n",
    "\n",
    "There's no need to use keep=False here, if anything is duplicated there will be at least one `True` value regardless of the value of `keep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a911821-f0de-43b1-92f7-6a14464dcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not borough.SegmentID.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e41a62-af43-4888-8390-e23331a11bf3",
   "metadata": {},
   "source": [
    "Now, we can finally do the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b234ca4-d69e-46ae-813b-c0874654b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(borough, on=\"SegmentID\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb92cc9-9964-427a-aa74-3971f03e816c",
   "metadata": {},
   "source": [
    "Now, let's check the quality of the merge, by seeing how many failed to merge, and inspecting those that failed to merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf57a27-97dc-42dd-aa7f-d1ce9c2ce1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.borough.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c359f7a-28eb-4450-8ff7-696326659c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data.borough.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468a1d0-9cfa-4072-83a6-e3a5c7f26e78",
   "metadata": {},
   "source": [
    "Unfortunately, this is quite a few streets (almost 10%) where the merge failed. If I were doing a more extensive project, I'd investigate this further, but for our purposes we'll just ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9db3a3-4fb7-4092-a9aa-0a3535bc4dcc",
   "metadata": {},
   "source": [
    "## Descriptive statistics\n",
    "\n",
    "Many descriptive statistics are available as functions of columns. For instance, we can use `.mean()` to calculate the mean number of vehicles that cross a sensor between 2 and 3 pm. We have to refer to the column using subscript notation (`[\"column\"]`) instead of the usual dot notation `.name` because the column names are not valid Python variable names (they start with numbers and include colons and dashes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c78582-3383-445e-8387-fbabcb255b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"2:00-3:00PM\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f80516-5ceb-4791-8a97-8767c0421fe7",
   "metadata": {},
   "source": [
    "## Grouped data analysis/split-apply-combine\n",
    "\n",
    "We can use `groupby` to split our dataset into pieces based on the values of some variable, and then apply analyses to each group individually. For instance, we can compute the mean number of vehicles crossing between 2 and 3 PM by sensor and direction. When groupby by multiple variables in `pandas`, you must enclose the group variables in a list (`[]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79bd92-9ab7-4081-9374-f083508aec2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby([\"SegmentID\", \"Direction\"])[\"2:00-3:00PM\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75a3f2-a43b-4eb5-a764-13173983f035",
   "metadata": {},
   "source": [
    "## Data types\n",
    "\n",
    "Every column in a `pandas` data frame has a data type. We can view these data types with the `.dtypes` property of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc94ef8-b16b-4a3b-ba73-99e4f38a5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdb426-c777-484c-aa13-ce6864937459",
   "metadata": {},
   "source": [
    "We see that all of the columns with counts are `float64` - or floating point (decimal) number. Looking at the data, you might expect these to be integers (`int64`), but `pandas` has autodetected them as `float64`. This commonly happens with numeric columns with missing data. Floating-point numbers allow `pandas` to represent missing data as `NaN` or not-a-number, which is a special value of floating point number. There is no such value for integers, which would force `pandas` to store the missing values separately, slightly increasing memory consumption.\n",
    "\n",
    "ID and SegmentID are `int64`, indicating that these are integers (as expected) with no missing values.\n",
    "\n",
    "Roadway Name, From, To, and Date are stored as `object` columns. These can represent any Python object, but are most often used to represent strings (text). We expect Roadway Name, From, and To to be text. However, Date should be stored as a date column if we want to do any analysis with it (e.g. sorting, selecting specific months, plotting, etc.). There are two ways to do this. The first is to parse the column, using `pd.to_datetime`. Specifying a format is optional, but I always like to do it to make sure that there is no confusion about whether the dates were month-day-year, year-month-day, etc. You can [see the documentation](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes) for what codes you can use in the format description.\n",
    "\n",
    "I like to print out the result before overwriting the variable, to make sure everything parsed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8f7e4b-0da1-489e-9913-a5a16175a365",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(data.Date, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261702d-434e-41c5-ac18-ec2b4ad4de06",
   "metadata": {},
   "source": [
    "Next, we can overwrite the Date field using the result of our `pd.to_datetime` call. We have to use subscript notation `[\"column\"]` here, because we are creating or overwriting a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350e313a-068d-45c1-81da-5e4350cf7d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date\"] = pd.to_datetime(data.Date, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85db0826-14ad-40a3-9aec-d953207963b1",
   "metadata": {},
   "source": [
    "Then, we can check the dtypes again to make sure it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b0db9-2fc9-4290-a354-e5c46a31b8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558f5758-6c5a-461c-baa3-249c28096563",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Often, plots and graphs are the most effective way to present data. The `matplotlib` library provides extensive functionality for plotting. Here, we will plot a histogram of the dates, and create a line plot of afternoon traffic by day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9174ee8-8a6f-48d5-83b4-ed47adae57d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.Date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf119d5-c415-4750-99ef-050d4df21c75",
   "metadata": {},
   "source": [
    "To plot average afternoon traffic by day, we first have to use a groupby to calculate average afternoon traffic, and then we can plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87775f8a-36f3-4c6d-9322-079a386ddb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_day = data.groupby(\"Date\")[\"4:00-5:00PM\"].mean().reset_index()\n",
    "by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf405a6-5e5c-4225-86cf-e9132e59c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(by_day.Date, by_day[\"4:00-5:00PM\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b948307-86d1-43a5-8e88-8cf42e07ea83",
   "metadata": {},
   "source": [
    "Why might this graph be misleading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cec3b-9786-4ff5-993d-175d1352c943",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
