{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6cac4e5-2ab8-4ad5-95d9-3c768fef23c6",
   "metadata": {},
   "source": [
    "# Combining Base Python and Pandas\n",
    "\n",
    "The base Python topics we covered in the previous notebook are common in other fields, but in social science most coding uses Pandas features. That said, having a knowledge of the base Python features are useful in combination with Pandas. That is what we cover in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00567160-cd66-4337-b9ea-67966979dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9283f7-5a3c-45ee-8cac-1c4d96e3c711",
   "metadata": {},
   "source": [
    "We will once again use the NY street data we used for the first activity.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Load that data, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbcfff-356c-41ed-b49d-4f3fda270510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Traffic_Volume_Counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae53f96-ed71-43c4-8a3f-6da3cdda45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = pd.read_csv(\"../data/Traffic_Borough.csv\")\n",
    "borough[\"borough\"] = borough.RBoro.replace({\n",
    "    1: \"Manhattan\",\n",
    "    2: \"Bronx\",\n",
    "    3: \"Brooklyn\",\n",
    "    4: \"Queens\",\n",
    "    5: \"Staten Island\"\n",
    "}).astype(\"category\")\n",
    "borough = borough.dropna(subset=[\"borough\"]).drop_duplicates()\n",
    "data = data.merge(borough, on=\"SegmentID\", how=\"left\", validate=\"m:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838eb93f-9b80-4703-b1bc-26f7fc6bacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5832f4e-76e6-4a41-9152-99c55ec3371d",
   "metadata": {},
   "source": [
    "Next, we need to parse our date column. Go ahead and do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89bda8-4e56-4ac2-a3b5-687c937e9e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date\"] = pd.to_datetime(data.Date, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52423bb-b6b8-420d-91cb-0dbe5cecb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Roadway Name\": \"roadway_name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35934c8f-f68a-4ba6-aef9-677a2ac0d88f",
   "metadata": {},
   "source": [
    "## List comprehensions\n",
    "\n",
    "Let's create a column that has the total flow for the whole day. We could do this by manually specifying every column like this:\n",
    "\n",
    "```python\n",
    "data[\"total\"] = data[\"12:00-1:00AM\"] + data[\"1:00-2:00AM\"] + data[\"2:00-3:00AM\"] . . .\n",
    "```\n",
    "\n",
    "but this is tedious. Instead, we can use a list comprehension to select all of these columns, and then use the `pandas` `.sum()` function to add them together.\n",
    "\n",
    "First, we select the columns, and use an assertion to verify that there are 24 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac256963-40f0-4570-aabd-017d98725640",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = [c for c in data.columns if c.endswith(\"AM\") or c.endswith(\"PM\")]\n",
    "assert len(time_columns) == 24\n",
    "time_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271cc01-0d21-4b51-a4ef-ea5ed52b21ba",
   "metadata": {},
   "source": [
    "Next, we sum them all up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3feaad0-c98b-47e3-8299-c636dea59498",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"total\"] = data[time_columns].sum(axis=\"columns\")\n",
    "data.total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffa8f9-f14c-43b2-a0ca-8a4361f8ea1a",
   "metadata": {},
   "source": [
    "Now, we can make a histogram. I am increasing the number of bins to get a more granular histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f552461b-8182-4514-9f6d-185ad5071e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.total, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce2705-f117-425b-9c98-630042da8109",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use list comprehensions to create separate `am_total` and `pm_total` columns. Plot both of them. Adjust the bin size of the histogram to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17364576-c463-40af-bfec-81335b7ce744",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"am_total\"] = data[[c for c in data.columns if c.endswith(\"AM\")]].sum(axis=\"columns\")\n",
    "data[\"pm_total\"] = data[[c for c in data.columns if c.endswith(\"PM\")]].sum(axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4e16c-ac69-4522-bb39-a1820783523c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.am_total, bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d61ed2-91fb-46e6-a19c-0690a73954a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data.pm_total, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3979e5-15d3-435d-a23b-909eccf83483",
   "metadata": {},
   "source": [
    "Many of the streets in this dataset were observed over the course of more than one day. Suppose we are tasked with creating plots that compare daily volumes for several sensors, we might want to average together all the days that a sensor was observed in a particular direction. We can do this with our `time_columns` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e0cf8-e177-4d2c-95d7-90ac6d6509c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_traffic = data.groupby([\"SegmentID\", \"Direction\"])[time_columns].mean()\n",
    "mean_daily_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d46466-0c4d-49f3-842b-6febc0507b8f",
   "metadata": {},
   "source": [
    "Now, perhaps we would like to plot the traffic over the course of a day. In introduction to Python, we discussed \"long\" and \"wide\" formats. The data we have here are in a \"wide\" format. To plot it, it will be easier to have it in a \"long\" format. We can use the `.stack` function to convert it to \"long\" format. The \"stack\" function takes the column names and makes them an additional level of the index, with a single column remaining containing the values. `.unstack` is the opposite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfb8a3-c627-49c1-a4cb-2d6237a61ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_traffic = mean_daily_traffic.stack()\n",
    "mean_daily_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e7d0f3-639f-472f-ac8c-d0736ca203ef",
   "metadata": {},
   "source": [
    "This is now a series with a MultiIndex (meaning that the index has multiple columns; we'll talk more about this when we go in-depth talking about indexing in the next exercise). For now, we just want to convert it back to a DataFrame. But notice that the time and count columns do not have names. We can fix this before we convert to a dataframe.\n",
    "\n",
    "Remember that this is a series, so it is technically only one column; the other three are part of the \"index\". We can rename the series entirely to represent that it contains counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287d3f6-e5ba-4981-a55e-abf4d6501d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_traffic.rename(\"mean_count\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab8482f-8861-4d35-ab8e-a94208dd1aa3",
   "metadata": {},
   "source": [
    "For the time column, we need to rename the index level, so we use the `set_names` function of the index. We could do this in place, but I didn't here, to demonstrate how the index works. The index is separate from the data in a Series or DataFrame, so setting the names generates a new index, not a completely new series. Thus, we need to assign it to `mean_daily_traffic.index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52365c7-cb23-4728-8da0-6f624d921b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_daily_traffic.index = mean_daily_traffic.index.set_names(\"time\", level=2)\n",
    "mean_daily_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59911bdd-bc84-4759-a38d-14c5f897a0fe",
   "metadata": {},
   "source": [
    "Now, we can plot this data. Let's plot [sensor 38636, on 125th St in Harlem](https://www.google.com/maps/place/40%C2%B048'38.0%22N+73%C2%B057'07.3%22W/@40.8105409,-73.9565137,17z/data=!3m1!4b1!4m13!1m8!3m7!1s0x89c24ac4ca59bc75:0xb6b907ef32da45c!2sRamona+Ave,+Staten+Island,+NY!3b1!8m2!3d40.5375141!4d-74.2014443!16s%2Fg%2F1v2kybb4!3m3!8m2!3d40.810541!4d-73.952029) in the eastbound and westbound directions. First, we will use indexing to select the east- and westbound data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a2992a-aea4-4f33-8005-1a9c5b9f52bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_traffic = mean_daily_traffic.loc[38636, \"EB\"]\n",
    "wb_traffic = mean_daily_traffic.loc[38636, \"WB\"]\n",
    "wb_traffic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c663e8-653b-4967-9461-97700de74fbf",
   "metadata": {},
   "source": [
    "Because this is still a series, as far as matplotlib is concerned it is just a single column of counts. So the y axis is just nb_traffic or sb_traffic; the x axis will be the index. Note that the sensor and direction levels of the index are gone, since we selected based on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce418b6-1889-4dae-bea8-88bd75a29d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eb_traffic.index, eb_traffic, label=\"Eastbound\")\n",
    "plt.plot(wb_traffic.index, wb_traffic, label=\"Westbound\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505f2cb-627b-4a16-8e41-9f8c3bfb1831",
   "metadata": {},
   "source": [
    "The labels are hard to read. We can rotate them to make it easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4208dda9-1273-4c22-8e41-5f4081b5b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eb_traffic.index, eb_traffic, label=\"Eastbound\")\n",
    "plt.plot(wb_traffic.index, wb_traffic, label=\"Westbound\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8988303d-1c7c-4c37-8cb5-c167272dfbd5",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "How would you describe the traffic pattern here?\n",
    "\n",
    "Plot the Henry Hudson Parkway between Dyckman St and the Henry Hudson Bridge (segment 132490 southbound and 189377 northbound). This is a major commuter route in and out of New York City. Are the traffic patterns any different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b61c8a4-62b7-4207-9711-cb85a067db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_traffic = mean_daily_traffic.loc[189377, \"NB\"]\n",
    "sb_traffic = mean_daily_traffic.loc[132490, \"SB\"]\n",
    "\n",
    "plt.plot(nb_traffic.index, nb_traffic, label=\"Northbound\")\n",
    "plt.plot(sb_traffic.index, sb_traffic, label=\"Southbound\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e1d7e-f131-4f61-8d5e-8aef3ab32e1e",
   "metadata": {},
   "source": [
    "## Loops\n",
    "\n",
    "Loops can be very useful to automate repetitive data analysis tasks. Loops can be used to loop over the rows of a data frame, or can be used to loop over something else, for instance a set of parameters to do sensitivity testing of a model. For instance, we might look at what percentage of the sensors are on roads named \"street\" rather than road, avenue, drive, etc. We could do this with a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1fbaf-0294-4e45-a338-62fb3bfcdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "streets = 0\n",
    "for row in data.itertuples():\n",
    "    if \"street\" in row.roadway_name.lower():\n",
    "        streets += 1\n",
    "        \n",
    "streets /= len(data)\n",
    "streets *= 100\n",
    "streets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c42c4-514b-4772-bbcd-fb575c1c21ee",
   "metadata": {},
   "source": [
    "Loops are also often useful when you want to run an analysis multiple times, but with different parameters - for instance, a sensitivity test of a model. Suppose we want to calculate the percentage of low-volume streets that NYC has put sensors on, but we have several possible definitions for \"low-volume\" - less than 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, or 10000 vehicles per day.\n",
    "\n",
    "We could write this out as ten separate calculations, of course, but we can also just put the cutoffs in a list and loop over them and perform the calculation for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c33dc-3bd1-4441-843e-d7c6b2f76004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, compute a mean by segment and direction\n",
    "by_segment = data.groupby([\"SegmentID\", \"Direction\"]).total.mean()\n",
    "\n",
    "cutoffs = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "for cutoff in cutoffs:\n",
    "    pct = (by_segment < cutoff).mean() * 100\n",
    "    print(f\"{cutoff}: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd0177-485f-4436-8c44-73d9aeb8464f",
   "metadata": {},
   "source": [
    "## Forget everything I just told you\n",
    "\n",
    "Loops are common in most other programming languages, and are common in Python outside of data analytics. However, loops are rare in code using `numpy` or `pandas`. This is because loops in Python carry a high computation overhead, and loops over more than a few thousands items can start to slow down your analysis. Many programmers also find vectorized operations easier to read.\n",
    "\n",
    "Instead, users are encouraged to use _vectorized operations_, which operate on an entire column/series simultaneously. For instance, we might reimplement the code computing which roads are called \"street\" using the `str.lower()` and `.str.contains()` function. This will return a boolean (true/false) Series of whether each name contains \"street\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c6b48-d2cd-458a-a738-4cd99a7db8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb405a99-acfc-4497-85f2-1c60e22ee377",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name.str.lower().str.contains(\"street\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c05c0d-edd5-4e4f-bf0b-a67c0f6a0de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name.str.lower().str.contains(\"street\").mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee8555-ddfc-4981-858c-0867b0080478",
   "metadata": {},
   "source": [
    "The previous code using a loop ran plenty fast, but if the dataset was much larger, it might not. The `.str.contains()` format will be much faster in a large dataset, because it uses `pandas` code written in C for the actual operation. In addition, many programmers find the vectorized style more clear - and in this case, I agree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812103b1-afe5-49cd-b00e-17f79038bce4",
   "metadata": {},
   "source": [
    "The loop over different cutoff values is different, because it is looping over a much smaller list - only 10 values. This would be fast even with a large dataset, because the computations use vectorized operations (the vectorized `<`). I would probably write this loop this way. However, some pandas users prefer to avoid loops whenever possible. You could also implement it, by first cutting the `by_segment` series into intervals, and then summing the proportion of the series in each interval. We can use pd.cut and value_counts for this.\n",
    "\n",
    "pd.cut needs its breakpoints to include the start and end of the data, so first we create a new cutoffs list, adding 0 to the start and infinity to the end. We use the * operator to include all of our existing cutoffs in between. * within a list definition will \"unwrap\" the list it refers to, adding each element to the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a948c3-ed88-4175-8953-e0819eb9770c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inclusive_cutoffs = [0, *cutoffs, np.inf]\n",
    "inclusive_cutoffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaee3b-f8d9-42ce-95e3-3a1689c44862",
   "metadata": {},
   "source": [
    "Now, we can use `pd.cut` to divide the data into intervals. `pd.cut` is a function that bins the data into intervals. This is really useful, for instance if you want to group by ranges of a continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7728b0f-0740-42d6-90b4-8c09baee9b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_data = pd.cut(by_segment, inclusive_cutoffs)\n",
    "interval_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c129ce7-e051-4a10-84c0-cea2288740d0",
   "metadata": {},
   "source": [
    "Next, we can use `value_counts` to get the count of the number of sensors in each interval. We use `normalize=True` to get a proportion.\n",
    "\n",
    "`.value_counts()` just returns the count of each unique value in the column it is called on. If normalize is True, it will return proportions rather than raw counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8ef6e1-2e2a-449d-bb37-2015801a0879",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions = interval_data.value_counts(normalize=True)\n",
    "proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4ca2e-3b36-4bc8-8869-99bbe1e313c7",
   "metadata": {},
   "source": [
    "The last step is to do a cumulative sum of the proportions within each category, and multiply by 100 to get percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39169719-4a80-4c1a-912d-0ac27f36f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions.cumsum() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d2d89-e618-4035-b2b1-2709bf218d12",
   "metadata": {},
   "source": [
    "Does this match what we got before? If it does not, what has gone wrong?\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Fix the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc277d-053a-47fa-b545-62f1ddf655ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportions.sort_index().cumsum() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0de29a-86b5-43f2-8893-5a4bbf23f741",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "First using a loop and then using vectorized operations, calculate the percentage of observations in `data` that have AM total traffic greater than PM total traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de2e3af-1635-4ba1-8d1b-786a88df936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_greater_am = 0\n",
    "for idx, row in data.iterrows():\n",
    "    if row.am_total > row.pm_total:\n",
    "        rows_with_greater_am += 1\n",
    "        \n",
    "rows_with_greater_am /= len(data)\n",
    "rows_with_greater_am *= 100\n",
    "rows_with_greater_am"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80224f4e-0972-4ea5-a879-1700c42b3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data.am_total > data.pm_total).mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7c8f4-bdba-456f-b5d4-e64d2e73ba57",
   "metadata": {},
   "source": [
    "Does this accurately represent the proportion of segments that have higher AM traffic? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569c85c-861a-4c34-9f29-d9860843b0fe",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Functions are useful in a wide range of scenarios, because they allow you to encapsulate and re-use the same code in many places. For instance, we could re-write our function that calculates the percentage of segments below a certain cutoff as a function, with the dataset and the cutoff as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf6c44-a612-484d-b4fb-cfeee5ecc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_below_cutoff(series, cutoff):\n",
    "    # be careful here that you're referring to the correct dataset. the by_segment\n",
    "    # data created above is available here as well, so using by_segment instead of series would always\n",
    "    # refer to that data, even if something else was passed as an argument to the\n",
    "    # function.\n",
    "    return (series < cutoff).mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf8e2-6837-41a9-9652-46054cf6030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_below_cutoff(by_segment, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f24ec4-68b4-4ff3-ae11-79a9f32e7feb",
   "metadata": {},
   "source": [
    "We can then run all of our cutoffs fairly easily, by writing them out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5527fb0-77e5-44ed-9ba7-2883ad867480",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_below_cutoff(by_segment, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c01a6ad-e8e3-4b75-9083-a7719fd97c4d",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Alternately, we could use a loop like we did before, but use the function to calculate the percentage. Go ahead and do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddd142-1667-4c5c-a463-e950c309b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cutoff in cutoffs:\n",
    "    pct = percent_below_cutoff(by_segment, cutoff)\n",
    "    print(f\"{cutoff}: {pct:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906791f-36be-439a-8734-555cd2c4b3c8",
   "metadata": {},
   "source": [
    "We can also easily apply this to a different dataset. For example, maybe we want to apply it to the full dataset instead of segment-level means, so we are calculating the percentage of monitored days, not the percentage of segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84632d-b126-4fd8-8991-ec2ee0d393fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_below_cutoff(data, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3bae0-8f57-44f3-8f10-da32e4a38291",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Why didn't that work?\n",
    "\n",
    "Hint: what is different between data and by_segment?\n",
    "\n",
    "Fix the error without modifying the function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a51a7-d5a9-4b75-8fa7-5a43d11c6931",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_below_cutoff(data.total, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfc002-acf1-45a6-8f24-adaf7cd5c40e",
   "metadata": {},
   "source": [
    "Functions are extremely useful when analysis gets more complex. For example, suppose we wanted to determine whether traffic is higher during certain parts of the year, but we weren't sure how granular we want the analysis to be. We could write a function that would perform this analysis, with an argument that specifies the granularity, and a conditional to change granularity. This function will take a dataset and a granularity, and will return a count of the total traffic recorded during each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ccb4e4-e53e-4a40-adaa-6a34e4cd7b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_totals(dataframe, granularity):\n",
    "    # Based on the granularity argument, we will define a variable grouping_val that contains the\n",
    "    # values we wish to group by - months, weeks, etc.\n",
    "    if granularity == \"month\":\n",
    "        grouping_val = dataframe.Date.dt.month\n",
    "    elif granularity == \"week\":\n",
    "        grouping_val = dataframe.Date.dt.weekofyear\n",
    "    elif granularity == \"quarter\":\n",
    "        grouping_val = dataframe.Date.dt.quarter\n",
    "    else:\n",
    "        # We haven't seen this before, but this will raise an exception (error) that will\n",
    "        # be displayed in the notebook if the function is called with something other than\n",
    "        # month, week, or quarter as the argument.\n",
    "        raise ValueError(f\"Unknown granularity {granularity}\")\n",
    "    \n",
    "    # adding .sort_index here so they come out in the order they occur in the year\n",
    "    return dataframe.groupby(grouping_val).total.sum().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3bb6b-4249-4a39-a05b-69def043dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_data = traffic_totals(data, \"month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67591ede-3a2b-4c84-907e-539b85e19285",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_data = traffic_totals(data, \"week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a851b316-ff02-41cc-9e5f-2c573b16bcee",
   "metadata": {},
   "source": [
    "We got a warning about Series.dt.weekofyear being _deprecated_, or planned for removal in a future release of Pandas. Go back to the function and modify it based on the suggestion in the warning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f3705-5867-400e-8c18-0e89a34ca0ac",
   "metadata": {},
   "source": [
    "And for completeness, we'll call the function with a granularity we didn't define, to see the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9486f10-78fb-4569-a992-0b2213a306b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_totals(data, \"day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7a859-ac1b-47d0-a619-8a4a204dc2e3",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Plot the monthly and weekly data on separate plots. Does it appear that there is a time of year that has more traffic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d62da6-c330-4cc4-a36d-b12729040bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monthly_data.index, monthly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b52bd-1495-458a-bd7f-3f5e27c17780",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weekly_data.index, weekly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0484f-abc4-4b98-a2e0-bb4bb645bceb",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "There's a huge spike in October. One possible explanation is that NYC DOT is deploying more counting equipment in October. Write a function that accepts a data frame and a granularity argument and returns a similar result, but with a count of the number of records in each period, rather than the total traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df24a2-6986-4c6e-a8a9-6904829b32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_obs_frequency(dataframe, granularity):\n",
    "    # Based on the granularity argument, we will define a variable grouping_val that contains the \n",
    "    if granularity == \"month\":\n",
    "        grouping_val = dataframe.Date.dt.month\n",
    "    elif granularity == \"week\":\n",
    "        grouping_val = dataframe.Date.dt.isocalendar().week\n",
    "    elif granularity == \"quarter\":\n",
    "        grouping_val = dataframe.Date.dt.quarter\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown granularity {granularity}\")\n",
    "    \n",
    "    # adding .sort_index here so they come out in the order they occur in the year\n",
    "    # you couls \n",
    "    return dataframe.groupby(grouping_val).size().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7419a65-ed39-4da0-899a-7361a6d9e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_freq = count_obs_frequency(data, \"month\")\n",
    "weekly_freq = count_obs_frequency(data, \"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac2951-bbe7-4927-9894-edbfb7705e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(monthly_freq.index, monthly_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62db06c-e12b-4c30-906a-7c9976a28f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(weekly_freq.index, weekly_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d88dc-33d0-4657-a172-27557b419701",
   "metadata": {},
   "source": [
    "Yes, it appears the spike is due to additional data collection, not additional traffic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc7ed0-c5a4-4de7-806f-7977dcd08f2b",
   "metadata": {},
   "source": [
    "## Lambda functions\n",
    "\n",
    "Lambda functions are most often used in conjunction with the `.apply()` function in Pandas. This lets you perform arbitrary operations on each item in a column or each row or column of a DataFrame. For instance, let's adjust the \"Roadway Name\" field to be in title case. The lambda function will be called once per record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d3fcb-54ee-45e3-97bf-cdf20748cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name.apply(lambda name: name.title())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8621e816-c646-4471-90e0-2b88c758a9c6",
   "metadata": {},
   "source": [
    "Using apply to process all rows of a data frame should be avoided if possible, because it causes the same performance penalties as using a loop. For instance, to capitalize all of the names, it would be more efficient to use `.str.title()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f74166f-acb6-423e-adcc-1fc84b468bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name.str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc492f01-a4a3-41f2-847e-b565db30b240",
   "metadata": {},
   "source": [
    "`apply()` can also be used with data frames. This is useful when you need to do some sort of analysis that involves multiple columns. For instance, perhaps we want to create a \"description\" field that includes the name of the street as well as the cross streets.\n",
    "\n",
    "When using `apply()` with data frames, you need to pass an `axis` argument, which tells `pandas` whether you want to apply your function to each row or to each column. The syntax of this is confusing - saying `axis=\"columns\"` will run your function once per row. The reasoning is that `axis=\"columns\"` means that every time your function runs, it has all columns for a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b64a95-6615-46ad-985d-b8e16a8eba9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.apply(lambda row: f\"{row.roadway_name} from {row.From} to {row.To}\", axis=\"columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b962212d-414b-4dfc-acd0-e75e4b2c9b7f",
   "metadata": {},
   "source": [
    "As before, this is something that would be quicker without using apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134f8131-059a-4408-8f4d-4606ef712022",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.roadway_name + \" from \" + row.From + \" to \" + row.To"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac4088e-5764-477a-bd34-b1d01e70fa76",
   "metadata": {},
   "source": [
    "Lastly, we can use apply to apply a function to every column. For example, let's find the interquartile range of the records at each time of day, using our by_segment dataset created above. Pandas does not have a built-in interquartile range function.\n",
    "\n",
    "Since the function is being applied to each column rather than each row, and there are usually a small number of columns (less than several thousand, anyhow), this is pretty fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6307eb-d927-4722-a82f-27623205e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[time_columns].apply(lambda column: column.quantile(0.75) - column.quantile(0.25), axis=\"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5a521-10a6-41f9-b75d-bb84c74dfae8",
   "metadata": {},
   "source": [
    "Some programmers would prefer to write this without using apply. You can do this with the quantile function of a dataframe, which also allows you to specify an axis, to compute the 25th and 75th quantiles of each column. This is a case where I actually prefer apply - there's not much performance penalty, and I find it much more readable. But that's just my personal preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fee4cd-0e2b-47e0-95c2-dbcd394868d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[time_columns].quantile(0.75, axis=\"rows\") - data[time_columns].quantile(0.25, axis=\"rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540bb389-7fd2-4918-9d7e-8d7982e79388",
   "metadata": {},
   "source": [
    "## Apply with groupby\n",
    "\n",
    "Probably the place I use `apply` most frequently is with groupby. Apply used in this context runs the function once per group. The usual concerns about overhead apply; this may or may not be an issue, depending on how many groups you have. This is especially useful when you have a calculation that involves multiple columns that you want to run once per group.\n",
    "\n",
    "Let's look at how well morning traffic predicts evening traffic, by borough. We'll group our data by borough, and then use `apply` to compute the correlation between 8-9 AM and 5-6 PM within each borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48a2a50-1a32-4290-92bd-857b6148909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"borough\").apply(lambda group: group[\"8:00-9:00AM\"].corr(group[\"5:00-6:00PM\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6199f-ce1f-4510-ba30-2590f74e324c",
   "metadata": {},
   "source": [
    "How well does morning traffic predict afternoon traffic? Are there any variations by borough?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f321c0-9c11-4354-ac48-78b5ee145271",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Use groupby and apply to compute what proportion of segments have higher traffic between 8 and 9 AM than they do between 5 and 6 PM, by borough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad0c9c-f60a-4e2f-9b6f-2bdd12c53bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(\"borough\").apply(lambda group: (group[\"8:00-9:00AM\"] > group[\"5:00-6:00PM\"]).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
